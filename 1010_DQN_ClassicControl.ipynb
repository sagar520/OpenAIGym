{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "$$ \\huge{\\underline{\\textbf{ Classical Control with DQN }}} $$\n",
    "\n",
    "$$ \\large{\\textbf{MountainCar - Pendulum - CartPole - Acrobot - MoonLander}} $$\n",
    "\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks presents basic **Deep Q-Network (DQN)** used to solve OpenAI Gym **Classic Control** environments like Mountain Car, Inverted Pendulum and so on. In this notebook we use DQN **without target network** for educational purposes. For practical applications use target network.\n",
    "\n",
    "The environments solved in this notebook are:\n",
    "* MountainCar-v0 - modified to remove 200 step time limit\n",
    "* Pendulum-v0 - modified with discrete actions and converted state space\n",
    "* CartPole-v0\n",
    "* Acrobot-v1\n",
    "* LunarLander-v2\n",
    "\n",
    "I tried to match pre-processing and hyper parameters for all the environments. As the result some hyperparams will differ from optimal. Special note goes to _frame skip_ parameter, which is set to 4 for mountain car and moon lander and omitted otherwise. It can be brought to value 2 for all environments and all of them should still train OK. As for training iterations, I used 25k everywhere with exception of LunarLander (200k). Epsilon decay is also elongated for lunar lander."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)  # init TF ...\n",
    "config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)  # w/o taking ...\n",
    "with tf.compat.v1.Session(config=config): pass            # all GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import helpers for plotting (source file: [helpers.py](helpers.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "import importlib\n",
    "importlib.reload(helpers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(st, model, eps):\n",
    "    if np.random.rand() > eps:\n",
    "        q_values = model.eval(np.stack([st]))\n",
    "        return np.argmax(q_values)\n",
    "    else:\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop. Supports resuming from pre-trained agent and callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, frames, gamma, eps_decay_steps, eps_target,\n",
    "               batch_size, model, mem, start_step=0,\n",
    "               callback=None, trace=None, render=False):\n",
    "    \"\"\"Q-Learning, supprots resume\n",
    "    \n",
    "    Note: If resuming, all parameters should be identical to original call, with\n",
    "        exception of 'start_step' and 'frames'.\n",
    "    \n",
    "    Params:\n",
    "        env - environment\n",
    "        frames - number of time steps to execute\n",
    "        gamma - discount factor [0..1]\n",
    "        eps_decay_steps - decay epsilon-greedy param over that many time steps\n",
    "        eps_target - epsilon-greedy param after decay\n",
    "        batch_size - neural network batch size from memory buffer\n",
    "        model      - function approximator, already initialised, with methods:\n",
    "                     eval(state, action) -> float\n",
    "                     train(state, target) -> None\n",
    "        mem - memory reply buffer\n",
    "        start_step - if continuning, pass in return value (tts_) here\n",
    "        callback - optional callback to execute\n",
    "        trace - this object handles data logging, plotting etc.\n",
    "        render - render openai gym environment?\n",
    "    \"\"\"\n",
    "    \n",
    "    def eps_schedule(tts, eps_decay_steps, eps_target):\n",
    "        if tts > eps_decay_steps:\n",
    "            return eps_target\n",
    "        else:\n",
    "            eps_per_step_change = (1-eps_target) / eps_decay_steps\n",
    "            return 1.0 - tts * eps_per_step_change\n",
    "    \n",
    "        \n",
    "    assert len(mem) >= batch_size\n",
    "    \n",
    "    tts_ = start_step                        # total time step\n",
    "    for _ in itertools.count():              # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        episode_reward = 0                   # purely for logging\n",
    "        if render: env.render()\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            eps = eps_schedule(tts_, eps_decay_steps, eps_target)\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            episode_reward += R\n",
    "            if render: env.render()\n",
    "            \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(tts_, t_, S, A, R, done, eps, episode_reward, model, mem, trace)\n",
    "            \n",
    "            states, actions, rewards, n_states, dones, _ = mem.get_batch(batch_size)\n",
    "            targets = model.eval(n_states)\n",
    "            targets = rewards + gamma * np.max(targets, axis=-1)\n",
    "            targets[dones] = rewards[dones]  # return of next-to-terminal state is just R\n",
    "            model.train(states, actions, targets)\n",
    "\n",
    "            S = S_\n",
    "            \n",
    "            tts_ += 1\n",
    "            if tts_ >= start_step + frames:\n",
    "                return tts_                  # so we can pick up where we left\n",
    "            \n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripped down version for evaluation. Does not train agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, model, frames=None, episodes=None, eps=0.0, render=False):\n",
    "    assert frames is not None or episodes is not None\n",
    "        \n",
    "    total_reward = 0\n",
    "    \n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        if episodes is not None and e_ >= episodes:\n",
    "            return total_reward\n",
    "        \n",
    "        S = env.reset()\n",
    "        if render: env.render()\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, _ = env.step(A)\n",
    "            total_reward += R\n",
    "            if render: env.render()\n",
    "    \n",
    "            S = S_\n",
    "            \n",
    "            tts_ += 1\n",
    "            if frames is not None and tts_ >= frames:\n",
    "                return\n",
    "            \n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripped down version to pre-fill memory buffer with random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_fill(env, mem, steps=None, episodes=None, render=False):\n",
    "        \n",
    "    # Fill memory buffer using random policy\n",
    "    tts_ = 0\n",
    "    for e_ in itertools.count():\n",
    "        if episodes is not None and e_ >= episodes:\n",
    "            return\n",
    "        \n",
    "        S = env.reset();\n",
    "        if render: env.render()\n",
    "        \n",
    "        for t_ in itertools.count():\n",
    "        \n",
    "            A = env.action_space.sample()    # random policy\n",
    "            S_, R, done, _ = env.step(A)\n",
    "            if render: env.render()\n",
    "                \n",
    "            mem.append(S, A, R, S_, done)\n",
    "            \n",
    "            S = S_\n",
    "            \n",
    "            tts_ += 1\n",
    "            if steps is not None and tts_ >= steps:\n",
    "                return\n",
    "            \n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Approximators and Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of simple neural network with two hidden layers. As as side note, for classic control tasks tilings work much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFNeuralNet():\n",
    "    def __init__(self, nb_in, nb_hid_1, nb_hid_2, nb_out, lr):\n",
    "        self.nb_in = nb_in\n",
    "        self.nb_hid_1 = nb_hid_1\n",
    "        self.nb_hid_2 = nb_hid_2\n",
    "        self.nb_out = nb_out\n",
    "        \n",
    "        tf.compat.v1.reset_default_graph()\n",
    "              \n",
    "        self._x = tf.compat.v1.placeholder(name='xx', shape=[None, nb_in], dtype=tf.float32)\n",
    "        self._y = tf.compat.v1.placeholder(name='yy', shape=[None, nb_out], dtype=tf.float32)\n",
    "\n",
    "        self._h_hid_1 = tf.compat.v1.layers.dense(self._x, units=nb_hid_1,\n",
    "                                        activation=tf.nn.relu, name='Hidden_1')\n",
    "        self._h_hid_2 = tf.compat.v1.layers.dense(self._h_hid_1, units=nb_hid_2,\n",
    "                                        activation=tf.nn.relu, name='Hidden_2')\n",
    "        self._y_hat = tf.compat.v1.layers.dense(self._h_hid_2, units=nb_out,\n",
    "                                      activation=None, name='Output')\n",
    "        self._loss = tf.losses.mean_squared_error(self._y, self._y_hat)\n",
    "\n",
    "        self._optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=lr)\n",
    "        self._train_op = self._optimizer.minimize(self._loss)\n",
    "\n",
    "        self._sess = tf.compat.v1.Session()\n",
    "        self._sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        \n",
    "    def backward(self, x, y):\n",
    "        assert x.ndim == y.ndim == 2\n",
    "        _, y_hat, loss = self._sess.run([self._train_op, self._y_hat, self._loss],\n",
    "                                         feed_dict={self._x: x, self._y:y})\n",
    "        return y_hat, loss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._sess.run(self._y_hat, feed_dict={self._x: x})\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self._sess, filepath)\n",
    "        \n",
    "    def load(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self._sess, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function approximator wrapper. Mostly performs sanity check and input/output normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFFunctApprox():\n",
    "\n",
    "    def __init__(self, model, st_low, st_high, rew_mean, rew_std, nb_actions):\n",
    "        \"\"\"Q-function approximator using Keras model\n",
    "\n",
    "        Args:\n",
    "            model: TFNeuralNet model\n",
    "        \"\"\"\n",
    "        st_low = np.array(st_low);\n",
    "        st_high = np.array(st_high)\n",
    "        self._model = model\n",
    "        \n",
    "        assert st_low.ndim == 1 and st_low.shape == st_high.shape\n",
    "        \n",
    "        if len(st_low) != model.nb_in:\n",
    "            raise ValueError('Input shape does not match state_space shape')\n",
    "\n",
    "        if nb_actions != model.nb_out:\n",
    "            raise ValueError('Output shape does not match action_space shape')\n",
    "\n",
    "        # normalise inputs\n",
    "        self._offsets = st_low + (st_high - st_low) / 2\n",
    "        self._scales = 1 / ((st_high - st_low) / 2)\n",
    "        \n",
    "        self._rew_mean = rew_mean\n",
    "        self._rew_std = rew_std\n",
    "\n",
    "    def eval(self, states):\n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "\n",
    "        y_hat = self._model.forward(inputs)\n",
    "        \n",
    "        #return y_hat\n",
    "        return y_hat*self._rew_std + self._rew_mean\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \n",
    "        assert isinstance(states, np.ndarray)\n",
    "        assert isinstance(actions, np.ndarray)\n",
    "        assert isinstance(targets, np.ndarray)\n",
    "        assert states.ndim == 2\n",
    "        assert actions.ndim == 1\n",
    "        assert targets.ndim == 1\n",
    "        assert len(states) == len(actions) == len(targets)\n",
    "        \n",
    "        \n",
    "        targets = (targets-self._rew_mean) / self._rew_std    # normalise\n",
    "\n",
    "        inputs = (states - self._offsets) * self._scales\n",
    "        all_targets = self._model.forward(inputs)       # this should normalised already\n",
    "        all_targets[np.arange(len(all_targets)), actions] = targets\n",
    "        self._model.backward(inputs, all_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory reply buffer. Reasonably fast impl., doesn't do memory copies when picking batches. This will be useful for Atari. Python deque is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Circular buffer for DQN memory reply. Fairly fast.\"\"\"\n",
    "\n",
    "    def __init__(self, max_len, state_shape, state_dtype):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_len: maximum capacity\n",
    "        \"\"\"\n",
    "        assert isinstance(max_len, int)\n",
    "        assert max_len > 0\n",
    "\n",
    "        self.max_len = max_len                      # maximum length        \n",
    "        self._curr_insert_ptr = 0                   # index to insert next data sample\n",
    "        self._curr_len = 0                          # number of currently stored elements\n",
    "\n",
    "        state_arr_shape = [max_len] + list(state_shape)\n",
    "\n",
    "        self._hist_St = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_At = np.zeros(max_len, dtype=int)\n",
    "        self._hist_Rt_1 = np.zeros(max_len, dtype=float)\n",
    "        self._hist_St_1 = np.zeros(state_arr_shape, dtype=state_dtype)\n",
    "        self._hist_done_1 = np.zeros(max_len, dtype=bool)\n",
    "\n",
    "    def append(self, St, At, Rt_1, St_1, done_1):\n",
    "        \"\"\"Add one sample to memory, override oldest if max_len reached.\n",
    "\n",
    "        Args:\n",
    "            St [np.ndarray]   - state\n",
    "            At [int]          - action\n",
    "            Rt_1 [float]      - reward\n",
    "            St_1 [np.ndarray] - next state\n",
    "            done_1 [bool]       - next state terminal?\n",
    "        \"\"\"\n",
    "        self._hist_St[self._curr_insert_ptr] = St\n",
    "        self._hist_At[self._curr_insert_ptr] = At\n",
    "        self._hist_Rt_1[self._curr_insert_ptr] = Rt_1\n",
    "        self._hist_St_1[self._curr_insert_ptr] = St_1\n",
    "        self._hist_done_1[self._curr_insert_ptr] = done_1\n",
    "        \n",
    "        if self._curr_len < self.max_len:                 # keep track of current length\n",
    "            self._curr_len += 1\n",
    "            \n",
    "        self._curr_insert_ptr += 1                        # increment insertion pointer\n",
    "        if self._curr_insert_ptr >= self.max_len:         # roll to zero if needed\n",
    "            self._curr_insert_ptr = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of samples in memory, 0 <= length <= max_len\"\"\"\n",
    "        return self._curr_len\n",
    "\n",
    "    def get_batch(self, batch_len):\n",
    "        \"\"\"Sample batch of data, with repetition\n",
    "\n",
    "        Args:\n",
    "            batch_len: nb of samples to pick\n",
    "\n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, next_done, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert self._curr_len > 0\n",
    "        assert batch_len > 0\n",
    "\n",
    "        \n",
    "        indices = np.random.randint(        # randint much faster than np.random.sample\n",
    "            low=0, high=self._curr_len, size=batch_len, dtype=int)\n",
    "\n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "\n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "\n",
    "\n",
    "    \n",
    "    def pick_last(self, nb):\n",
    "        \"\"\"Pick last nb elements from memory\n",
    "        \n",
    "        Returns:\n",
    "            states, actions, rewards, next_states, done_1, indices\n",
    "            Each returned element is np.ndarray with length == batch_len\n",
    "        \"\"\"\n",
    "        assert nb <= self._curr_len\n",
    "        \n",
    "        start = self._curr_insert_ptr - nb                # inclusive\n",
    "        end = self._curr_insert_ptr                       # not inclusive\n",
    "        indices = np.array(range(start,end), dtype=int)   # indices to pick, can be neg.\n",
    "        indices[indices < 0] += self._curr_len            # loop negative to positive\n",
    "        \n",
    "        states = np.take(self._hist_St, indices, axis=0)\n",
    "        actions = np.take(self._hist_At, indices, axis=0)\n",
    "        rewards_1 = np.take(self._hist_Rt_1, indices, axis=0)\n",
    "        states_1 = np.take(self._hist_St_1, indices, axis=0)\n",
    "        dones_1 = np.take(self._hist_done_1, indices, axis=0)\n",
    "        \n",
    "        return states, actions, rewards_1, states_1, dones_1, indices\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment wrapper to skip frames. Also known as _action repeat_. Really helps with some environments, e.g. mountain car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapFrameSkip():\n",
    "    def __init__(self, env, frameskip):\n",
    "        assert frameskip >= 1\n",
    "        self._env = env\n",
    "        self._frameskip = frameskip\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "    \n",
    "    def reset(self):\n",
    "        return self._env.reset()\n",
    "    \n",
    "    def step(self, action):\n",
    "        sum_rew = 0\n",
    "        for _ in range(self._frameskip):\n",
    "            obs, rew, done, info = self._env.step(action)\n",
    "            sum_rew += rew\n",
    "            if done: break\n",
    "        return obs, sum_rew, done, info\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self._env.render(mode=mode)\n",
    "        \n",
    "    def close(self):\n",
    "        self._env.close()               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object is a basic data logger. If you don't want to generate plots, you can mostly ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trace():\n",
    "    def __init__(self, eval_every, render=False, test_states=None, state_labels=None):\n",
    "        \n",
    "        if test_states is not None:\n",
    "            assert test_states.ndim == 2\n",
    "            \n",
    "        self.enable_plotting = False\n",
    "        \n",
    "        self.eval_every = eval_every\n",
    "        self.test_states = test_states\n",
    "        self.state_labels = state_labels\n",
    "        \n",
    "        self.tstep = 0\n",
    "        self.total_tstep = 0\n",
    "        \n",
    "        self.q_values = collections.OrderedDict()\n",
    "        self.ep_rewards = collections.defaultdict(float)\n",
    "        self.last_ep_reward = None\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []  # t+1\n",
    "        self.dones = []    # t+1\n",
    "        self.epsilons = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback which is called every single training iteration, it does as follows:\n",
    "* fill up trace object with basic logging data\n",
    "* print training progress\n",
    "* perform evaluation of `test_states`\n",
    "* optionally plot agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(total_time_step, tstep, st, act, rew_, done_,\n",
    "             eps, ep_reward, model, memory, trace):\n",
    "    \"\"\"Called from gradient_MC after every episode.\n",
    "    \n",
    "    Params:\n",
    "        episode [int] - episode number\n",
    "        tstep [int]   - timestep within episode\n",
    "        model [obj]   - function approximator\n",
    "        trace [list]  - list to write results to\"\"\"\n",
    "    \n",
    "    assert total_time_step == trace.total_tstep\n",
    "    \n",
    "    trace.tstep = tstep\n",
    "    \n",
    "    trace.states.append(st)\n",
    "    trace.actions.append(act)\n",
    "    trace.rewards.append(rew_)\n",
    "    trace.dones.append(done_)\n",
    "    trace.epsilons.append(eps)\n",
    "        \n",
    "    if done_:\n",
    "        trace.ep_rewards[total_time_step] = ep_reward\n",
    "        trace.last_ep_reward = ep_reward\n",
    "            \n",
    "    #\n",
    "    #   Print, Evaluate, Plot\n",
    "    #\n",
    "    if (trace.eval_every is not None) and (trace.total_tstep % trace.eval_every == 0):\n",
    "        \n",
    "        last_ep_rew = trace.last_ep_reward\n",
    "        reward_str = str(round(last_ep_rew, 3)) if last_ep_rew is not None else 'None'\n",
    "        print(f'wall: {datetime.datetime.now().strftime(\"%H:%M:%S\")}   '\n",
    "              f'ep: {len(trace.ep_rewards):3}   tstep: {tstep:4}   '\n",
    "              f'total tstep: {trace.total_tstep:6}   '\n",
    "              f'eps: {eps:5.3f}   reward: {reward_str}   ')\n",
    "\n",
    "        if len(st) == 2:\n",
    "            # We are working with 2D environment,\n",
    "            # eval. Q-Value function across whole state space\n",
    "            q_arr = helpers.eval_state_action_space(model, env, split=[128,128])\n",
    "            trace.q_values[trace.total_tstep] = q_arr\n",
    "        else:\n",
    "            # Environment is not 2D,\n",
    "            # eval. on pre-defined random sample of states\n",
    "            if trace.test_states is not None:\n",
    "                y_hat = model.eval(trace.test_states)\n",
    "                trace.q_values[trace.total_tstep] = y_hat\n",
    "\n",
    "        if trace.enable_plotting:\n",
    "            helpers.plot_all(env, model, memory, trace)\n",
    "            print('■'*80)\n",
    "\n",
    "    trace.total_tstep += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with mountain car environment\n",
    "\n",
    "Notes:\n",
    "* OpenAI gym imposes **200 step limit**, which makes this environment *extremely* hard to solve with DQN - we remove this limit\n",
    "* Frame-skip (aka action repeat) is technically not necessary, but makes training *much* easier, good value is frameskip=4\n",
    "\n",
    "The nut and bolt of mountain car is that agent gets extremely sparse reward on successful exit. Shortest possible exit trajectory is approx 120-130 steps. If we impose 200 step limit and don't repeat actions this means agent has to perform series of 120-130 actions exactly correctly in the row with very little margin for error. If agent doesn't succeed it gets reset back to beginning on step 200. To do this with random exploratory policy is virtually impossible.\n",
    "\n",
    "Note that this can still be solved easily with linear function approximation (local updates) and optimistic initialisation which facilitates continuing exploration. This is not possible with DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_mountaincar():\n",
    "    neural_net = TFNeuralNet(nb_in=2, nb_hid_1=64, nb_hid_2=64, nb_out=3, lr=0.00025)\n",
    "    \n",
    "    model = TFFunctApprox(neural_net,\n",
    "                          env.observation_space.low,\n",
    "                          env.observation_space.high,\n",
    "                          rew_mean=-50,\n",
    "                          rew_std=15,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(2,), state_dtype=float)\n",
    "    mem_fill(env, mem, steps=10000)\n",
    "    test_states, _, _, _, _, _ = mem.get_batch(10)\n",
    "    \n",
    "    trace = Trace(eval_every=1000, test_states=test_states)\n",
    "    \n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0').env  # remove 200 step limit\n",
    "env = WrapFrameSkip(env, frameskip=4)\n",
    "trace, model, mem = experiment_mountaincar()\n",
    "# trace.enable_plotting = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall: 16:32:14   ep:   0   tstep:    0   total tstep:      0   eps: 1.000   reward: None   \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'helpers' has no attribute 'eval_state_action_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-b07856538c9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m tts = q_learning(env, frames=25000, gamma=.99,\n\u001b[0;32m      2\u001b[0m                  \u001b[0meps_decay_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_target\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                  model=model, mem=mem, callback=callback, trace=trace)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-b028f65e2c52>\u001b[0m in \u001b[0;36mq_learning\u001b[1;34m(env, frames, gamma, eps_decay_steps, eps_target, batch_size, model, mem, start_step, callback, trace, render)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtts_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-940209dca010>\u001b[0m in \u001b[0;36mcallback\u001b[1;34m(total_time_step, tstep, st, act, rew_, done_, eps, ep_reward, model, memory, trace)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m# We are working with 2D environment,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m# eval. Q-Value function across whole state space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mq_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_state_action_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_tstep\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'helpers' has no attribute 'eval_state_action_space'"
     ]
    }
   ],
   "source": [
    "tts = q_learning(env, frames=25000, gamma=.99,\n",
    "                 eps_decay_steps=20000, eps_target=0.1, batch_size=4096,\n",
    "                 model=model, mem=mem, callback=callback, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: train some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts = q_learning(env, frames=5000, gamma=.99,\n",
    "#                  eps_decay_steps=20000, eps_target=0.1, batch_size=4096,\n",
    "#                  model=model, mem=mem, start_step=tts, callback=callback, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: plot the agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers.plot_all(env, model, mem, trace, print_=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.save('./tf_models/MountainCar.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_models/MountainCar.ckpt\n"
     ]
    }
   ],
   "source": [
    "model._model.load('./tf_models/MountainCar.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter, press squre '■' in top menu to quit animation\n",
    "try: evaluate(env, model, frames=float('inf'), eps=0.0, render=True)\n",
    "except KeyboardInterrupt: pass\n",
    "finally: env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected agent behaviour after 15000 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/mountaincar.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enable plotting, then output after training should be in roughly as follows\n",
    "\n",
    "```\n",
    "wall: 16:45:30   ep:  91   tstep:   22   total tstep:  15000   eps: 0.325   reward: -134.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/mountaincar.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "* Q_Max - is a plot of q-function max over possible actions\n",
    "* Trajectory - is top-down view of agent trajectory. Agent starts roughly in the centre and exits to the right. Colour indicates action, see policy plot\n",
    "* Q Values - is a plot of average q-values for set or randomly selected states - this shows how q-values develop during training\n",
    "* Policy - is a top-down plot of agent policy. Green is accelerate right, red is accelerate left, blue is neutral\n",
    "* Memory Buffer - is a snapshot of memory buffer, same format as Trajectory plot\n",
    "* Episode Rewards - individual rewards acquired (blue dots) and running average (yellow) over the training period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pendulum is in my opinion actually quite a bit easier than mountain car. Mainly because of rich reward signal.\n",
    "\n",
    "Notes:\n",
    "* OpenAI pendulum has continuous actions while basic DQN requires discrete actions - we will discretise it\n",
    "* by default state space is 3-vector: [cos(angle), sin(angle), velocity] - to be able to plot whole state space we reduce it to [angle, velocity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pendulum2DEnv():\n",
    "    def __init__(self):\n",
    "        self._env = gym.make('Pendulum-v0')\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([-np.pi, -8.0]), high=np.array([np.pi, 8.0]), dtype=np.float32 )\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        \n",
    "    def reset(self):\n",
    "        cos, sin, vel = self._env.reset()\n",
    "        theta = np.arctan2(sin, cos)\n",
    "        return np.array([theta, vel])\n",
    "        \n",
    "    def step(self, action):\n",
    "        torques = [-2.0, 0.0, 2.0]\n",
    "        # torques = [-2.0, -.5, 0.0, .5, 2.0]\n",
    "        joint_effort = torques[action]\n",
    "        \n",
    "        obs, rew, done, _ = self._env.step([joint_effort])\n",
    "        cos, sin, vel = obs\n",
    "        theta = np.arctan2(sin, cos)\n",
    "        return np.array([theta, vel]), rew, done, obs\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self._env.render(mode=mode)\n",
    "        \n",
    "    def close(self):\n",
    "        self._env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_pendulum():\n",
    "    neural_net = TFNeuralNet(nb_in=2, nb_hid_1=64, nb_hid_2=64, nb_out=3, lr=0.00025)\n",
    "    \n",
    "    model = TFFunctApprox(neural_net,\n",
    "                          env.observation_space.low,\n",
    "                          env.observation_space.high,\n",
    "                          rew_mean=-210,\n",
    "                          rew_std=50,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(2,), state_dtype=float)\n",
    "    mem_fill(env, mem, steps=10000)\n",
    "    test_states, _, _, _, _, _ = mem.get_batch(10)\n",
    "    \n",
    "    trace = Trace(eval_every=1000, test_states=test_states)\n",
    "    \n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-56c0ba575313>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPendulum2DEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment_pendulum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# trace.enable_plotting = True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-99882b121c0c>\u001b[0m in \u001b[0;36mexperiment_pendulum\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mexperiment_pendulum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mneural_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFNeuralNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_in\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_hid_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_hid_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00025\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     model = TFFunctApprox(neural_net,\n\u001b[0;32m      5\u001b[0m                           \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-1430bc2c4ac2>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, nb_in, nb_hid_1, nb_hid_2, nb_out, lr)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'xx'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
     ]
    }
   ],
   "source": [
    "env = Pendulum2DEnv()\n",
    "trace, model, mem = experiment_pendulum()\n",
    "# trace.enable_plotting = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall: 10:11:12   ep:   0   tstep:    0   total tstep:      0   eps: 1.000   reward: None   \n",
      "wall: 10:11:16   ep:   5   tstep:    0   total tstep:   1000   eps: 0.955   reward: -932.75   \n",
      "wall: 10:11:21   ep:  10   tstep:    0   total tstep:   2000   eps: 0.910   reward: -1349.562   \n",
      "wall: 10:11:26   ep:  15   tstep:    0   total tstep:   3000   eps: 0.865   reward: -1182.014   \n",
      "wall: 10:11:31   ep:  20   tstep:    0   total tstep:   4000   eps: 0.820   reward: -765.12   \n",
      "wall: 10:11:36   ep:  25   tstep:    0   total tstep:   5000   eps: 0.775   reward: -758.568   \n",
      "wall: 10:11:41   ep:  30   tstep:    0   total tstep:   6000   eps: 0.730   reward: -971.488   \n",
      "wall: 10:11:47   ep:  35   tstep:    0   total tstep:   7000   eps: 0.685   reward: -867.18   \n",
      "wall: 10:11:52   ep:  40   tstep:    0   total tstep:   8000   eps: 0.640   reward: -1113.014   \n",
      "wall: 10:11:57   ep:  45   tstep:    0   total tstep:   9000   eps: 0.595   reward: -662.024   \n",
      "wall: 10:12:03   ep:  50   tstep:    0   total tstep:  10000   eps: 0.550   reward: -527.786   \n",
      "wall: 10:12:08   ep:  55   tstep:    0   total tstep:  11000   eps: 0.505   reward: -370.842   \n",
      "wall: 10:12:14   ep:  60   tstep:    0   total tstep:  12000   eps: 0.460   reward: -254.44   \n",
      "wall: 10:12:19   ep:  65   tstep:    0   total tstep:  13000   eps: 0.415   reward: -370.899   \n",
      "wall: 10:12:24   ep:  70   tstep:    0   total tstep:  14000   eps: 0.370   reward: -252.474   \n",
      "wall: 10:12:30   ep:  75   tstep:    0   total tstep:  15000   eps: 0.325   reward: -123.758   \n",
      "wall: 10:12:36   ep:  80   tstep:    0   total tstep:  16000   eps: 0.280   reward: -524.958   \n",
      "wall: 10:12:41   ep:  85   tstep:    0   total tstep:  17000   eps: 0.235   reward: -123.024   \n",
      "wall: 10:12:47   ep:  90   tstep:    0   total tstep:  18000   eps: 0.190   reward: -128.003   \n",
      "wall: 10:12:52   ep:  95   tstep:    0   total tstep:  19000   eps: 0.145   reward: -119.915   \n",
      "wall: 10:12:57   ep: 100   tstep:    0   total tstep:  20000   eps: 0.100   reward: -116.671   \n",
      "wall: 10:13:03   ep: 105   tstep:    0   total tstep:  21000   eps: 0.100   reward: -350.965   \n",
      "wall: 10:13:08   ep: 110   tstep:    0   total tstep:  22000   eps: 0.100   reward: -3.165   \n",
      "wall: 10:13:14   ep: 115   tstep:    0   total tstep:  23000   eps: 0.100   reward: -241.848   \n",
      "wall: 10:13:20   ep: 120   tstep:    0   total tstep:  24000   eps: 0.100   reward: -125.71   \n"
     ]
    }
   ],
   "source": [
    "tts = q_learning(env, frames=25000, gamma=.99,\n",
    "                 eps_decay_steps=20000, eps_target=0.1, batch_size=4096,\n",
    "                 model=model, mem=mem, callback=callback, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: train some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts = q_learning(env, frames=5000, gamma=.99,\n",
    "#                  eps_decay_steps=20000, eps_target=0.1, batch_size=4096,\n",
    "#                  model=model, mem=mem, start_step=tts, callback=callback, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: plot the agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers.plot_all(env, model, mem, trace, print_=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.save('./tf_models/Pendulum.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_models/Pendulum.ckpt\n"
     ]
    }
   ],
   "source": [
    "model._model.load('./tf_models/Pendulum.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter, press squre '■' in top menu to quit animation\n",
    "try: evaluate(env, model, frames=float('inf'), eps=0.0, render=True)\n",
    "except KeyboardInterrupt: pass\n",
    "finally: env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected agent behaviour after 25000 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/pendulum.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enable plotting, then output after training should be in roughly as follows\n",
    "\n",
    "```\n",
    "wall: 08:13:49   ep: 250   tstep:   99   total tstep:  25000   eps: 0.100   reward: -227.530\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/pendulum.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "* Q_Max - is a plot of q-function max over possible actions\n",
    "* Trajectory - is top-down view of agent trajectory. Agent starts roughly in the centre and exits to the right. Colour indicates action, see policy plot\n",
    "* Q Values - is a plot of average q-values for set or randomly selected states - this shows how q-values develop during training\n",
    "* Policy - is a top-down plot of agent policy. Green is accelerate right, red is accelerate left, blue is neutral\n",
    "* Memory Buffer - is a snapshot of memory buffer, same format as Trajectory plot\n",
    "* Episode Rewards - individual rewards acquired (blue dots) and running average (yellow) over the training period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everybody loves cartpole!\n",
    "\n",
    "Notes:\n",
    "* this environment no longer has 2d state space so we can't plot it all :( - as a result plots look differently\n",
    "\n",
    "This environment has peculiar dynamic where positive reward, along with non-corrected maximisation bias in Q-Function (we don't use Dual-DQN) causes runaway Q-Value effect where Q-Values increase continuously. This is ok for now, because RELATIVE Q-Values are still good enough to select correct action. I think using target network along with Dual-DQN would help with this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_cartpole():\n",
    "    neural_net = TFNeuralNet(nb_in=4, nb_hid_1=64, nb_hid_2=64, nb_out=2, lr=0.00025)\n",
    "    \n",
    "    model = TFFunctApprox(neural_net,\n",
    "                          st_low=np.array([-.2, -1.0, -0.15, -1.0]),\n",
    "                          st_high=np.array([.2, 1.0, 0.15, 1.0]),\n",
    "                          rew_mean=0,\n",
    "                          rew_std=1,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(4,), state_dtype=float)\n",
    "    mem_fill(env, mem, steps=10000)\n",
    "    test_states, _, _, _, _, _ = mem.get_batch(10)\n",
    "    \n",
    "    trace = Trace(eval_every=1000, test_states=test_states)\n",
    "    \n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env = gym.make('CartPole-v0')\n",
    "trace, model, mem = experiment_cartpole()\n",
    "# trace.enable_plotting = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall: 10:14:17   ep:   0   tstep:    0   total tstep:      0   eps: 1.000   reward: None   \n",
      "wall: 10:14:21   ep:  47   tstep:    1   total tstep:   1000   eps: 0.955   reward: 9.0   \n",
      "wall: 10:14:26   ep:  90   tstep:   10   total tstep:   2000   eps: 0.910   reward: 47.0   \n",
      "wall: 10:14:31   ep: 131   tstep:    5   total tstep:   3000   eps: 0.865   reward: 27.0   \n",
      "wall: 10:14:36   ep: 156   tstep:   68   total tstep:   4000   eps: 0.820   reward: 29.0   \n",
      "wall: 10:14:42   ep: 181   tstep:    5   total tstep:   5000   eps: 0.775   reward: 41.0   \n",
      "wall: 10:14:47   ep: 205   tstep:   71   total tstep:   6000   eps: 0.730   reward: 16.0   \n",
      "wall: 10:14:52   ep: 225   tstep:   50   total tstep:   7000   eps: 0.685   reward: 31.0   \n",
      "wall: 10:14:57   ep: 241   tstep:  151   total tstep:   8000   eps: 0.640   reward: 10.0   \n",
      "wall: 10:15:03   ep: 255   tstep:   93   total tstep:   9000   eps: 0.595   reward: 80.0   \n",
      "wall: 10:15:08   ep: 267   tstep:   55   total tstep:  10000   eps: 0.550   reward: 134.0   \n",
      "wall: 10:15:13   ep: 276   tstep:  144   total tstep:  11000   eps: 0.505   reward: 101.0   \n",
      "wall: 10:15:18   ep: 285   tstep:   38   total tstep:  12000   eps: 0.460   reward: 194.0   \n",
      "wall: 10:15:24   ep: 292   tstep:   74   total tstep:  13000   eps: 0.415   reward: 200.0   \n",
      "wall: 10:15:29   ep: 298   tstep:   16   total tstep:  14000   eps: 0.370   reward: 200.0   \n",
      "wall: 10:15:34   ep: 304   tstep:   68   total tstep:  15000   eps: 0.325   reward: 143.0   \n",
      "wall: 10:15:40   ep: 309   tstep:   68   total tstep:  16000   eps: 0.280   reward: 200.0   \n",
      "wall: 10:15:45   ep: 314   tstep:  125   total tstep:  17000   eps: 0.235   reward: 200.0   \n",
      "wall: 10:15:51   ep: 319   tstep:  125   total tstep:  18000   eps: 0.190   reward: 200.0   \n",
      "wall: 10:15:56   ep: 324   tstep:  125   total tstep:  19000   eps: 0.145   reward: 200.0   \n",
      "wall: 10:16:02   ep: 329   tstep:  125   total tstep:  20000   eps: 0.100   reward: 200.0   \n",
      "wall: 10:16:07   ep: 334   tstep:  125   total tstep:  21000   eps: 0.100   reward: 200.0   \n",
      "wall: 10:16:13   ep: 339   tstep:  125   total tstep:  22000   eps: 0.100   reward: 200.0   \n",
      "wall: 10:16:19   ep: 344   tstep:  125   total tstep:  23000   eps: 0.100   reward: 200.0   \n",
      "wall: 10:16:24   ep: 349   tstep:  125   total tstep:  24000   eps: 0.100   reward: 200.0   \n"
     ]
    }
   ],
   "source": [
    "tts = q_learning(env, frames=25000, gamma=.99,\n",
    "                 eps_decay_steps=20000, eps_target=0.1, batch_size=4096,\n",
    "                 model=model, mem=mem, callback=callback, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: train some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts = q_learning(env, frames=5000, gamma=.99,\n",
    "#                  eps_decay_steps=20000, eps_target=0.1, batch_size=4096,\n",
    "#                  model=model, mem=mem, start_step=tts, callback=callback, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: plot the agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers.plot_all(env, model, mem, trace, print_=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.save('./tf_models/CartPole.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_models/CartPole.ckpt\n"
     ]
    }
   ],
   "source": [
    "model._model.load('./tf_models/CartPole.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter, press squre '■' in top menu to quit animation\n",
    "try: evaluate(env, model, frames=float('inf'), eps=0.0, render=True)\n",
    "except KeyboardInterrupt: pass\n",
    "finally: env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected agent behaviour after 25000 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/cartpole.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enable plotting, then output after training should be in roughly as follows\n",
    "\n",
    "```\n",
    "wall: 10:16:24   ep: 349   tstep:  125   total tstep:  24000   eps: 0.100   reward: 200.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/cartpole.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "* Q Values - is a plot of average q-values for set of states - this shows how q-values develop during training\n",
    "* Episode Rewards - individual rewards acquired (blue dots) and running average (yellow) over the training period\n",
    "* Trajectory - is set of state variables over time within one or more episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a variation of pendulum environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_acrobot():\n",
    "    neural_net = TFNeuralNet(nb_in=6, nb_hid_1=64, nb_hid_2=64, nb_out=3, lr=0.00025)\n",
    "    \n",
    "    model = TFFunctApprox(neural_net,\n",
    "                          env.observation_space.low,\n",
    "                          env.observation_space.high,\n",
    "                          rew_mean=-70,\n",
    "                          rew_std=10,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(6,), state_dtype=float)\n",
    "    mem_fill(env, mem, steps=10000)\n",
    "    test_states, _, _, _, _, _ = mem.get_batch(10)\n",
    "    \n",
    "    trace = Trace(eval_every=1000, test_states=test_states)\n",
    "    \n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = env = gym.make('Acrobot-v1')\n",
    "trace, model, mem = experiment_acrobot()\n",
    "# trace.enable_plotting = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall: 10:18:58   ep:   0   tstep:    0   total tstep:      0   eps: 1.000   reward: None   \n",
      "wall: 10:19:03   ep:   2   tstep:    0   total tstep:   1000   eps: 0.955   reward: -500.0   \n",
      "wall: 10:19:09   ep:   4   tstep:    0   total tstep:   2000   eps: 0.910   reward: -500.0   \n",
      "wall: 10:19:15   ep:   6   tstep:    0   total tstep:   3000   eps: 0.865   reward: -500.0   \n",
      "wall: 10:19:20   ep:   8   tstep:    0   total tstep:   4000   eps: 0.820   reward: -500.0   \n",
      "wall: 10:19:26   ep:  10   tstep:  270   total tstep:   5000   eps: 0.775   reward: -287.0   \n",
      "wall: 10:19:32   ep:  13   tstep:   25   total tstep:   6000   eps: 0.730   reward: -354.0   \n",
      "wall: 10:19:38   ep:  16   tstep:   56   total tstep:   7000   eps: 0.685   reward: -337.0   \n",
      "wall: 10:19:44   ep:  20   tstep:   61   total tstep:   8000   eps: 0.640   reward: -286.0   \n",
      "wall: 10:19:50   ep:  25   tstep:   54   total tstep:   9000   eps: 0.595   reward: -241.0   \n",
      "wall: 10:19:55   ep:  30   tstep:  100   total tstep:  10000   eps: 0.550   reward: -163.0   \n",
      "wall: 10:20:01   ep:  36   tstep:   88   total tstep:  11000   eps: 0.505   reward: -118.0   \n",
      "wall: 10:20:07   ep:  42   tstep:  145   total tstep:  12000   eps: 0.460   reward: -175.0   \n",
      "wall: 10:20:13   ep:  49   tstep:  106   total tstep:  13000   eps: 0.415   reward: -105.0   \n",
      "wall: 10:20:19   ep:  56   tstep:  116   total tstep:  14000   eps: 0.370   reward: -133.0   \n",
      "wall: 10:20:26   ep:  64   tstep:   76   total tstep:  15000   eps: 0.325   reward: -141.0   \n",
      "wall: 10:20:32   ep:  70   tstep:   30   total tstep:  16000   eps: 0.280   reward: -147.0   \n",
      "wall: 10:20:38   ep:  77   tstep:   46   total tstep:  17000   eps: 0.235   reward: -120.0   \n",
      "wall: 10:20:44   ep:  84   tstep:  141   total tstep:  18000   eps: 0.190   reward: -142.0   \n",
      "wall: 10:20:51   ep:  93   tstep:   74   total tstep:  19000   eps: 0.145   reward: -100.0   \n",
      "wall: 10:20:57   ep: 103   tstep:   80   total tstep:  20000   eps: 0.100   reward: -87.0   \n",
      "wall: 10:21:03   ep: 114   tstep:   44   total tstep:  21000   eps: 0.100   reward: -88.0   \n",
      "wall: 10:21:10   ep: 126   tstep:   59   total tstep:  22000   eps: 0.100   reward: -83.0   \n",
      "wall: 10:21:16   ep: 133   tstep:  153   total tstep:  23000   eps: 0.100   reward: -153.0   \n",
      "wall: 10:21:23   ep: 142   tstep:   82   total tstep:  24000   eps: 0.100   reward: -105.0   \n"
     ]
    }
   ],
   "source": [
    "tts = q_learning(env, frames=25000, gamma=.99,\n",
    "                 eps_decay_steps=20000, eps_target=0.1, batch_size=4096,\n",
    "                 model=model, mem=mem, callback=callback, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: train some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts = q_learning(env, frames=5000, gamma=.99,\n",
    "#                  eps_decay_steps=20000, eps_target=0.1, batch_size=4096,\n",
    "#                  model=model, mem=mem, start_step=tts, callback=callback, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: plot the agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers.plot_all(env, model, mem, trace, print_=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.save('./tf_models/Acrobot.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_models/Acrobot.ckpt\n"
     ]
    }
   ],
   "source": [
    "model._model.load('./tf_models/Acrobot.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter, press squre '■' in top menu to quit animation\n",
    "try: evaluate(env, model, frames=float('inf'), eps=0.0, render=True)\n",
    "except KeyboardInterrupt: pass\n",
    "finally: env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected agent behaviour after 25000 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/acrobot.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enable plotting, then output after training should be in roughly as follows\n",
    "\n",
    "```\n",
    "wall: 09:23:11   ep: 332   tstep:   31   total tstep:  24000   eps: 0.100   reward: -90.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/acrobot.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "* Q Values - is a plot of average q-values for set of states - this shows how q-values develop during training\n",
    "* Episode Rewards - individual rewards acquired (blue dots) and running average (yellow) over the training period\n",
    "* Trajectory - is set of state variables over time within one or more episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_lunarlander():\n",
    "    neural_net = TFNeuralNet(nb_in=8, nb_hid_1=64, nb_hid_2=64, nb_out=4, lr=0.00025)\n",
    "    \n",
    "    model = TFFunctApprox(neural_net,\n",
    "                          st_low=np.array([-1., -1., -1., -1., -1., -1., -1., -1.]),\n",
    "                          st_high=np.array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]),\n",
    "                          rew_mean=0,\n",
    "                          rew_std=1,\n",
    "                          nb_actions=env.action_space.n)\n",
    "    \n",
    "    mem = Memory(max_len=100000, state_shape=(8,), state_dtype=float)\n",
    "    mem_fill(env, mem, steps=10000)\n",
    "    \n",
    "    trace = Trace(eval_every=1000,\n",
    "                  test_states=np.array([[0, 1.4, 0, 0, 0, 0, 0, 0],     # init\n",
    "                                        [0, 0.7, 0, 0, 0, 0, 0, 0],     # half way\n",
    "                                        [0, 0.0, 0, 0, 0, 0, 0, 0],]),  # landing pad\n",
    "                  state_labels=['Pos.x', 'Pos.y', 'Vel.x', 'Vel.y',\n",
    "                                'Angle', 'Ang. Vel', 'Left Leg', 'Right Leg'])\n",
    "    \n",
    "    return trace, model, mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "#env = WrapFrameSkip(env, frameskip=4)\n",
    "trace, model, mem = experiment_lunarlander()\n",
    "# trace.enable_plotting = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall: 10:22:41   ep:   0   tstep:    0   total tstep:      0   eps: 1.000   reward: None   \n",
      "wall: 10:22:46   ep:  10   tstep:   20   total tstep:   1000   eps: 0.982   reward: -115.29   \n",
      "wall: 10:22:52   ep:  21   tstep:   31   total tstep:   2000   eps: 0.964   reward: -271.343   \n",
      "wall: 10:22:58   ep:  31   tstep:   33   total tstep:   3000   eps: 0.946   reward: -218.339   \n",
      "wall: 10:23:03   ep:  40   tstep:  101   total tstep:   4000   eps: 0.928   reward: -206.543   \n",
      "wall: 10:23:09   ep:  52   tstep:   18   total tstep:   5000   eps: 0.910   reward: -112.62   \n",
      "wall: 10:23:15   ep:  63   tstep:   30   total tstep:   6000   eps: 0.892   reward: -101.06   \n",
      "wall: 10:23:21   ep:  74   tstep:   13   total tstep:   7000   eps: 0.874   reward: -188.096   \n",
      "wall: 10:23:27   ep:  83   tstep:   27   total tstep:   8000   eps: 0.856   reward: -126.304   \n",
      "wall: 10:23:33   ep:  93   tstep:   92   total tstep:   9000   eps: 0.838   reward: -106.536   \n",
      "wall: 10:23:39   ep: 104   tstep:   19   total tstep:  10000   eps: 0.820   reward: -73.858   \n",
      "wall: 10:23:45   ep: 113   tstep:   33   total tstep:  11000   eps: 0.802   reward: -295.832   \n",
      "wall: 10:23:51   ep: 124   tstep:   81   total tstep:  12000   eps: 0.784   reward: -77.515   \n",
      "wall: 10:23:57   ep: 133   tstep:  111   total tstep:  13000   eps: 0.766   reward: -132.166   \n",
      "wall: 10:24:03   ep: 142   tstep:   16   total tstep:  14000   eps: 0.748   reward: -69.937   \n",
      "wall: 10:24:10   ep: 146   tstep:  650   total tstep:  15000   eps: 0.730   reward: -46.965   \n",
      "wall: 10:24:16   ep: 153   tstep:   35   total tstep:  16000   eps: 0.712   reward: -81.782   \n",
      "wall: 10:24:23   ep: 160   tstep:  137   total tstep:  17000   eps: 0.694   reward: -230.064   \n",
      "wall: 10:24:29   ep: 168   tstep:   45   total tstep:  18000   eps: 0.676   reward: -25.934   \n",
      "wall: 10:24:35   ep: 177   tstep:   47   total tstep:  19000   eps: 0.658   reward: -77.265   \n",
      "wall: 10:24:41   ep: 186   tstep:   36   total tstep:  20000   eps: 0.640   reward: -78.109   \n",
      "wall: 10:24:48   ep: 193   tstep:  130   total tstep:  21000   eps: 0.622   reward: -22.37   \n",
      "wall: 10:24:54   ep: 201   tstep:  135   total tstep:  22000   eps: 0.604   reward: -0.823   \n",
      "wall: 10:25:00   ep: 210   tstep:    8   total tstep:  23000   eps: 0.586   reward: -39.496   \n",
      "wall: 10:25:07   ep: 214   tstep:  577   total tstep:  24000   eps: 0.568   reward: -26.854   \n",
      "wall: 10:25:14   ep: 216   tstep:  706   total tstep:  25000   eps: 0.550   reward: -34.765   \n",
      "wall: 10:25:22   ep: 217   tstep:  706   total tstep:  26000   eps: 0.532   reward: 111.061   \n",
      "wall: 10:25:29   ep: 222   tstep:  154   total tstep:  27000   eps: 0.514   reward: -107.679   \n",
      "wall: 10:25:36   ep: 226   tstep:  131   total tstep:  28000   eps: 0.496   reward: -298.987   \n",
      "wall: 10:25:43   ep: 230   tstep:  218   total tstep:  29000   eps: 0.478   reward: -209.019   \n",
      "wall: 10:25:49   ep: 235   tstep:   75   total tstep:  30000   eps: 0.460   reward: -169.042   \n",
      "wall: 10:25:56   ep: 239   tstep:   24   total tstep:  31000   eps: 0.442   reward: -161.023   \n",
      "wall: 10:26:03   ep: 242   tstep:  379   total tstep:  32000   eps: 0.424   reward: -45.139   \n",
      "wall: 10:26:11   ep: 244   tstep:  333   total tstep:  33000   eps: 0.406   reward: -30.302   \n",
      "wall: 10:26:19   ep: 245   tstep:  996   total tstep:  34000   eps: 0.388   reward: -182.147   \n",
      "wall: 10:26:27   ep: 246   tstep:  996   total tstep:  35000   eps: 0.370   reward: -2.856   \n",
      "wall: 10:26:34   ep: 250   tstep:  187   total tstep:  36000   eps: 0.352   reward: -3.681   \n",
      "wall: 10:26:41   ep: 251   tstep:  496   total tstep:  37000   eps: 0.334   reward: -287.423   \n",
      "wall: 10:26:50   ep: 252   tstep:  496   total tstep:  38000   eps: 0.316   reward: -25.345   \n",
      "wall: 10:26:58   ep: 253   tstep:  496   total tstep:  39000   eps: 0.298   reward: 5.965   \n",
      "wall: 10:27:05   ep: 255   tstep:  325   total tstep:  40000   eps: 0.280   reward: -91.694   \n",
      "wall: 10:27:14   ep: 256   tstep:  325   total tstep:  41000   eps: 0.262   reward: -15.338   \n",
      "wall: 10:27:21   ep: 257   tstep:  325   total tstep:  42000   eps: 0.244   reward: 4.851   \n",
      "wall: 10:27:30   ep: 258   tstep:  325   total tstep:  43000   eps: 0.226   reward: 44.291   \n",
      "wall: 10:27:37   ep: 259   tstep:  325   total tstep:  44000   eps: 0.208   reward: 144.273   \n",
      "wall: 10:27:46   ep: 260   tstep:  325   total tstep:  45000   eps: 0.190   reward: 94.646   \n",
      "wall: 10:27:54   ep: 261   tstep:  325   total tstep:  46000   eps: 0.172   reward: 28.112   \n",
      "wall: 10:28:02   ep: 262   tstep:  325   total tstep:  47000   eps: 0.154   reward: 62.216   \n",
      "wall: 10:28:10   ep: 263   tstep:  399   total tstep:  48000   eps: 0.136   reward: -167.148   \n",
      "wall: 10:28:18   ep: 264   tstep:  740   total tstep:  49000   eps: 0.118   reward: 216.762   \n",
      "wall: 10:28:27   ep: 265   tstep:  740   total tstep:  50000   eps: 0.100   reward: 116.882   \n",
      "wall: 10:28:35   ep: 266   tstep:  740   total tstep:  51000   eps: 0.100   reward: 110.934   \n",
      "wall: 10:28:43   ep: 267   tstep:  740   total tstep:  52000   eps: 0.100   reward: 37.194   \n",
      "wall: 10:28:52   ep: 268   tstep:  740   total tstep:  53000   eps: 0.100   reward: -26.579   \n",
      "wall: 10:29:00   ep: 269   tstep:  740   total tstep:  54000   eps: 0.100   reward: 45.556   \n",
      "wall: 10:29:09   ep: 270   tstep:  740   total tstep:  55000   eps: 0.100   reward: 38.381   \n",
      "wall: 10:29:17   ep: 271   tstep:  740   total tstep:  56000   eps: 0.100   reward: -55.859   \n",
      "wall: 10:29:25   ep: 272   tstep:  740   total tstep:  57000   eps: 0.100   reward: -37.105   \n",
      "wall: 10:29:33   ep: 274   tstep:  232   total tstep:  58000   eps: 0.100   reward: 255.776   \n",
      "wall: 10:29:42   ep: 275   tstep:  232   total tstep:  59000   eps: 0.100   reward: -13.596   \n",
      "wall: 10:29:50   ep: 276   tstep:  232   total tstep:  60000   eps: 0.100   reward: 21.605   \n",
      "wall: 10:29:59   ep: 277   tstep:  956   total tstep:  61000   eps: 0.100   reward: -178.159   \n",
      "wall: 10:30:08   ep: 278   tstep:  956   total tstep:  62000   eps: 0.100   reward: -20.156   \n",
      "wall: 10:30:17   ep: 279   tstep:  956   total tstep:  63000   eps: 0.100   reward: 35.256   \n",
      "wall: 10:30:26   ep: 280   tstep:  956   total tstep:  64000   eps: 0.100   reward: 24.137   \n",
      "wall: 10:30:34   ep: 281   tstep:  956   total tstep:  65000   eps: 0.100   reward: 36.587   \n",
      "wall: 10:30:42   ep: 282   tstep:  956   total tstep:  66000   eps: 0.100   reward: 45.871   \n",
      "wall: 10:30:49   ep: 284   tstep:  392   total tstep:  67000   eps: 0.100   reward: 215.709   \n",
      "wall: 10:30:58   ep: 285   tstep:  392   total tstep:  68000   eps: 0.100   reward: 90.141   \n",
      "wall: 10:31:06   ep: 286   tstep:  931   total tstep:  69000   eps: 0.100   reward: 160.203   \n",
      "wall: 10:31:14   ep: 287   tstep:  931   total tstep:  70000   eps: 0.100   reward: 81.186   \n",
      "wall: 10:31:22   ep: 289   tstep:    4   total tstep:  71000   eps: 0.100   reward: 209.053   \n",
      "wall: 10:31:31   ep: 290   tstep:    4   total tstep:  72000   eps: 0.100   reward: 18.395   \n",
      "wall: 10:31:40   ep: 291   tstep:    4   total tstep:  73000   eps: 0.100   reward: 73.769   \n",
      "wall: 10:31:48   ep: 292   tstep:    4   total tstep:  74000   eps: 0.100   reward: 118.818   \n",
      "wall: 10:31:56   ep: 293   tstep:    4   total tstep:  75000   eps: 0.100   reward: 79.98   \n",
      "wall: 10:32:04   ep: 294   tstep:  667   total tstep:  76000   eps: 0.100   reward: 260.134   \n",
      "wall: 10:32:12   ep: 295   tstep:  667   total tstep:  77000   eps: 0.100   reward: 81.324   \n",
      "wall: 10:32:20   ep: 296   tstep:  667   total tstep:  78000   eps: 0.100   reward: 121.995   \n",
      "wall: 10:32:29   ep: 297   tstep:  667   total tstep:  79000   eps: 0.100   reward: 42.91   \n",
      "wall: 10:32:37   ep: 298   tstep:  667   total tstep:  80000   eps: 0.100   reward: 108.989   \n",
      "wall: 10:32:46   ep: 299   tstep:  667   total tstep:  81000   eps: 0.100   reward: 132.24   \n",
      "wall: 10:32:54   ep: 300   tstep:  667   total tstep:  82000   eps: 0.100   reward: 56.982   \n",
      "wall: 10:33:02   ep: 301   tstep:  667   total tstep:  83000   eps: 0.100   reward: 96.389   \n",
      "wall: 10:33:10   ep: 302   tstep:  667   total tstep:  84000   eps: 0.100   reward: 70.521   \n",
      "wall: 10:33:19   ep: 303   tstep:  667   total tstep:  85000   eps: 0.100   reward: 86.212   \n",
      "wall: 10:33:27   ep: 304   tstep:  667   total tstep:  86000   eps: 0.100   reward: 107.187   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall: 10:33:35   ep: 305   tstep:  667   total tstep:  87000   eps: 0.100   reward: 93.593   \n",
      "wall: 10:33:44   ep: 306   tstep:  667   total tstep:  88000   eps: 0.100   reward: 72.983   \n",
      "wall: 10:33:52   ep: 307   tstep:  667   total tstep:  89000   eps: 0.100   reward: 94.76   \n",
      "wall: 10:34:00   ep: 308   tstep:  667   total tstep:  90000   eps: 0.100   reward: 39.705   \n",
      "wall: 10:34:09   ep: 309   tstep:  938   total tstep:  91000   eps: 0.100   reward: 131.125   \n",
      "wall: 10:34:16   ep: 311   tstep:  544   total tstep:  92000   eps: 0.100   reward: -50.488   \n",
      "wall: 10:34:24   ep: 312   tstep:  544   total tstep:  93000   eps: 0.100   reward: -13.787   \n",
      "wall: 10:34:32   ep: 313   tstep:  544   total tstep:  94000   eps: 0.100   reward: 120.073   \n",
      "wall: 10:34:41   ep: 314   tstep:  544   total tstep:  95000   eps: 0.100   reward: 91.413   \n",
      "wall: 10:34:48   ep: 315   tstep:  544   total tstep:  96000   eps: 0.100   reward: 112.728   \n",
      "wall: 10:34:57   ep: 316   tstep:  786   total tstep:  97000   eps: 0.100   reward: 193.951   \n",
      "wall: 10:35:05   ep: 317   tstep:  786   total tstep:  98000   eps: 0.100   reward: 36.57   \n",
      "wall: 10:35:13   ep: 318   tstep:  924   total tstep:  99000   eps: 0.100   reward: 153.127   \n",
      "wall: 10:35:21   ep: 319   tstep:  924   total tstep: 100000   eps: 0.100   reward: 80.755   \n",
      "wall: 10:35:29   ep: 321   tstep:  506   total tstep: 101000   eps: 0.100   reward: 15.686   \n",
      "wall: 10:35:36   ep: 322   tstep:  506   total tstep: 102000   eps: 0.100   reward: 162.326   \n",
      "wall: 10:35:45   ep: 323   tstep:  602   total tstep: 103000   eps: 0.100   reward: 194.451   \n",
      "wall: 10:35:52   ep: 325   tstep:   44   total tstep: 104000   eps: 0.100   reward: 208.562   \n",
      "wall: 10:36:00   ep: 326   tstep:   44   total tstep: 105000   eps: 0.100   reward: 88.54   \n",
      "wall: 10:36:08   ep: 327   tstep:   44   total tstep: 106000   eps: 0.100   reward: 127.951   \n",
      "wall: 10:36:15   ep: 328   tstep:  641   total tstep: 107000   eps: 0.100   reward: 192.668   \n",
      "wall: 10:36:23   ep: 329   tstep:  641   total tstep: 108000   eps: 0.100   reward: 48.12   \n",
      "wall: 10:36:31   ep: 331   tstep:   99   total tstep: 109000   eps: 0.100   reward: 237.257   \n",
      "wall: 10:36:39   ep: 332   tstep:   99   total tstep: 110000   eps: 0.100   reward: -10.114   \n",
      "wall: 10:36:48   ep: 333   tstep:   99   total tstep: 111000   eps: 0.100   reward: 48.975   \n",
      "wall: 10:36:56   ep: 335   tstep:   52   total tstep: 112000   eps: 0.100   reward: -304.066   \n",
      "wall: 10:37:03   ep: 336   tstep:  616   total tstep: 113000   eps: 0.100   reward: 220.41   \n",
      "wall: 10:37:11   ep: 337   tstep:  917   total tstep: 114000   eps: 0.100   reward: 147.43   \n",
      "wall: 10:37:19   ep: 338   tstep:  971   total tstep: 115000   eps: 0.100   reward: 188.87   \n",
      "wall: 10:37:26   ep: 339   tstep:  971   total tstep: 116000   eps: 0.100   reward: 35.325   \n",
      "wall: 10:37:33   ep: 341   tstep:  460   total tstep: 117000   eps: 0.100   reward: 133.817   \n",
      "wall: 10:37:41   ep: 342   tstep:  460   total tstep: 118000   eps: 0.100   reward: 74.843   \n",
      "wall: 10:37:49   ep: 344   tstep:  616   total tstep: 119000   eps: 0.100   reward: 2.771   \n",
      "wall: 10:37:56   ep: 346   tstep:   53   total tstep: 120000   eps: 0.100   reward: 150.66   \n",
      "wall: 10:38:03   ep: 348   tstep:  569   total tstep: 121000   eps: 0.100   reward: -60.741   \n",
      "wall: 10:38:11   ep: 350   tstep:  164   total tstep: 122000   eps: 0.100   reward: 222.941   \n",
      "wall: 10:38:18   ep: 352   tstep:  383   total tstep: 123000   eps: 0.100   reward: 291.767   \n",
      "wall: 10:38:26   ep: 354   tstep:   26   total tstep: 124000   eps: 0.100   reward: 205.84   \n",
      "wall: 10:38:34   ep: 355   tstep:  388   total tstep: 125000   eps: 0.100   reward: 205.104   \n",
      "wall: 10:38:42   ep: 357   tstep:  290   total tstep: 126000   eps: 0.100   reward: 265.188   \n",
      "wall: 10:38:49   ep: 359   tstep:  325   total tstep: 127000   eps: 0.100   reward: 246.072   \n",
      "wall: 10:38:56   ep: 362   tstep:  228   total tstep: 128000   eps: 0.100   reward: 261.931   \n",
      "wall: 10:39:04   ep: 364   tstep:  212   total tstep: 129000   eps: 0.100   reward: 224.424   \n",
      "wall: 10:39:11   ep: 365   tstep:  750   total tstep: 130000   eps: 0.100   reward: 241.802   \n",
      "wall: 10:39:19   ep: 367   tstep:  325   total tstep: 131000   eps: 0.100   reward: 171.074   \n",
      "wall: 10:39:26   ep: 369   tstep:   55   total tstep: 132000   eps: 0.100   reward: 12.651   \n",
      "wall: 10:39:33   ep: 370   tstep:  874   total tstep: 133000   eps: 0.100   reward: -53.672   \n",
      "wall: 10:39:41   ep: 372   tstep:  586   total tstep: 134000   eps: 0.100   reward: 262.182   \n",
      "wall: 10:39:48   ep: 375   tstep:   82   total tstep: 135000   eps: 0.100   reward: 257.8   \n",
      "wall: 10:39:56   ep: 378   tstep:   11   total tstep: 136000   eps: 0.100   reward: -228.043   \n",
      "wall: 10:40:03   ep: 380   tstep:  284   total tstep: 137000   eps: 0.100   reward: 225.393   \n",
      "wall: 10:40:10   ep: 383   tstep:  122   total tstep: 138000   eps: 0.100   reward: 17.938   \n",
      "wall: 10:40:18   ep: 384   tstep:  122   total tstep: 139000   eps: 0.100   reward: 81.568   \n",
      "wall: 10:40:25   ep: 389   tstep:   38   total tstep: 140000   eps: 0.100   reward: 295.639   \n",
      "wall: 10:40:33   ep: 390   tstep:   38   total tstep: 141000   eps: 0.100   reward: 101.856   \n",
      "wall: 10:40:40   ep: 394   tstep:  152   total tstep: 142000   eps: 0.100   reward: -29.112   \n",
      "wall: 10:40:47   ep: 395   tstep:  152   total tstep: 143000   eps: 0.100   reward: 29.062   \n",
      "wall: 10:40:55   ep: 396   tstep:  362   total tstep: 144000   eps: 0.100   reward: 211.48   \n",
      "wall: 10:41:03   ep: 398   tstep:  291   total tstep: 145000   eps: 0.100   reward: -194.895   \n",
      "wall: 10:41:10   ep: 401   tstep:  117   total tstep: 146000   eps: 0.100   reward: -204.992   \n",
      "wall: 10:41:18   ep: 402   tstep:  117   total tstep: 147000   eps: 0.100   reward: 90.494   \n",
      "wall: 10:41:25   ep: 405   tstep:   73   total tstep: 148000   eps: 0.100   reward: 162.904   \n",
      "wall: 10:41:32   ep: 406   tstep:  714   total tstep: 149000   eps: 0.100   reward: 181.402   \n",
      "wall: 10:41:40   ep: 408   tstep:  310   total tstep: 150000   eps: 0.100   reward: 260.454   \n",
      "wall: 10:41:47   ep: 411   tstep:   48   total tstep: 151000   eps: 0.100   reward: 268.048   \n",
      "wall: 10:41:54   ep: 412   tstep:  572   total tstep: 152000   eps: 0.100   reward: 217.864   \n",
      "wall: 10:42:02   ep: 415   tstep:  269   total tstep: 153000   eps: 0.100   reward: 259.907   \n",
      "wall: 10:42:09   ep: 419   tstep:   71   total tstep: 154000   eps: 0.100   reward: -105.182   \n",
      "wall: 10:42:16   ep: 421   tstep:  122   total tstep: 155000   eps: 0.100   reward: 233.517   \n",
      "wall: 10:42:24   ep: 423   tstep:   25   total tstep: 156000   eps: 0.100   reward: -311.613   \n",
      "wall: 10:42:32   ep: 424   tstep:  445   total tstep: 157000   eps: 0.100   reward: 234.478   \n",
      "wall: 10:42:39   ep: 427   tstep:   39   total tstep: 158000   eps: 0.100   reward: 226.218   \n",
      "wall: 10:42:47   ep: 428   tstep:  107   total tstep: 159000   eps: 0.100   reward: 242.43   \n",
      "wall: 10:42:54   ep: 429   tstep:  522   total tstep: 160000   eps: 0.100   reward: 242.87   \n",
      "wall: 10:43:01   ep: 430   tstep:  522   total tstep: 161000   eps: 0.100   reward: 146.522   \n",
      "wall: 10:43:08   ep: 432   tstep:  753   total tstep: 162000   eps: 0.100   reward: 255.673   \n",
      "wall: 10:43:16   ep: 434   tstep:  492   total tstep: 163000   eps: 0.100   reward: 262.537   \n",
      "wall: 10:43:22   ep: 436   tstep:  289   total tstep: 164000   eps: 0.100   reward: 217.805   \n",
      "wall: 10:43:29   ep: 439   tstep:  200   total tstep: 165000   eps: 0.100   reward: 235.555   \n",
      "wall: 10:43:37   ep: 442   tstep:   30   total tstep: 166000   eps: 0.100   reward: 215.799   \n",
      "wall: 10:43:44   ep: 444   tstep:  240   total tstep: 167000   eps: 0.100   reward: 274.606   \n",
      "wall: 10:43:51   ep: 445   tstep:  240   total tstep: 168000   eps: 0.100   reward: 94.49   \n",
      "wall: 10:43:59   ep: 446   tstep:  796   total tstep: 169000   eps: 0.100   reward: 234.705   \n",
      "wall: 10:44:06   ep: 449   tstep:   60   total tstep: 170000   eps: 0.100   reward: 117.05   \n",
      "wall: 10:44:13   ep: 451   tstep:  199   total tstep: 171000   eps: 0.100   reward: 249.601   \n",
      "wall: 10:44:21   ep: 453   tstep:  144   total tstep: 172000   eps: 0.100   reward: 188.276   \n",
      "wall: 10:44:29   ep: 454   tstep:  144   total tstep: 173000   eps: 0.100   reward: 38.014   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wall: 10:44:37   ep: 456   tstep:   73   total tstep: 174000   eps: 0.100   reward: 274.643   \n",
      "wall: 10:44:44   ep: 457   tstep:  458   total tstep: 175000   eps: 0.100   reward: 235.782   \n",
      "wall: 10:44:52   ep: 459   tstep:  178   total tstep: 176000   eps: 0.100   reward: 224.325   \n",
      "wall: 10:45:00   ep: 460   tstep:  828   total tstep: 177000   eps: 0.100   reward: 206.592   \n",
      "wall: 10:45:07   ep: 463   tstep:   20   total tstep: 178000   eps: 0.100   reward: 203.23   \n",
      "wall: 10:45:14   ep: 465   tstep:  105   total tstep: 179000   eps: 0.100   reward: 242.451   \n",
      "wall: 10:45:22   ep: 466   tstep:  557   total tstep: 180000   eps: 0.100   reward: 213.594   \n",
      "wall: 10:45:29   ep: 468   tstep:  482   total tstep: 181000   eps: 0.100   reward: 260.273   \n",
      "wall: 10:45:36   ep: 471   tstep:   36   total tstep: 182000   eps: 0.100   reward: 254.961   \n",
      "wall: 10:45:44   ep: 473   tstep:  173   total tstep: 183000   eps: 0.100   reward: 237.072   \n",
      "wall: 10:45:51   ep: 474   tstep:  302   total tstep: 184000   eps: 0.100   reward: 168.218   \n",
      "wall: 10:45:59   ep: 477   tstep:    1   total tstep: 185000   eps: 0.100   reward: 254.415   \n",
      "wall: 10:46:07   ep: 478   tstep:  205   total tstep: 186000   eps: 0.100   reward: 166.87   \n",
      "wall: 10:46:14   ep: 480   tstep:   23   total tstep: 187000   eps: 0.100   reward: 219.167   \n",
      "wall: 10:46:22   ep: 481   tstep:   23   total tstep: 188000   eps: 0.100   reward: 121.224   \n",
      "wall: 10:46:29   ep: 482   tstep:  398   total tstep: 189000   eps: 0.100   reward: 233.602   \n",
      "wall: 10:46:37   ep: 483   tstep:  398   total tstep: 190000   eps: 0.100   reward: 94.814   \n",
      "wall: 10:46:45   ep: 485   tstep:  332   total tstep: 191000   eps: 0.100   reward: -101.423   \n",
      "wall: 10:46:52   ep: 486   tstep:  332   total tstep: 192000   eps: 0.100   reward: 110.194   \n",
      "wall: 10:47:00   ep: 487   tstep:  332   total tstep: 193000   eps: 0.100   reward: 50.283   \n",
      "wall: 10:47:07   ep: 490   tstep:  115   total tstep: 194000   eps: 0.100   reward: 261.25   \n",
      "wall: 10:47:14   ep: 491   tstep:  454   total tstep: 195000   eps: 0.100   reward: 229.827   \n",
      "wall: 10:47:21   ep: 493   tstep:  501   total tstep: 196000   eps: 0.100   reward: 227.933   \n",
      "wall: 10:47:29   ep: 494   tstep:  501   total tstep: 197000   eps: 0.100   reward: 130.779   \n",
      "wall: 10:47:37   ep: 496   tstep:  149   total tstep: 198000   eps: 0.100   reward: 221.184   \n",
      "wall: 10:47:44   ep: 498   tstep:   10   total tstep: 199000   eps: 0.100   reward: -65.76   \n"
     ]
    }
   ],
   "source": [
    "tts = q_learning(env, frames=200000, gamma=.99,\n",
    "                 eps_decay_steps=50000, eps_target=0.1, batch_size=4096,\n",
    "                 model=model, mem=mem, callback=callback, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: train some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts = q_learning(env, frames=5000, gamma=.99,\n",
    "#                  eps_decay_steps=50000, eps_target=0.1, batch_size=4096,\n",
    "#                  model=model, mem=mem, start_step=tts, callback=callback, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: plot the agent state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers.plot_all(env, model, mem, trace, print_=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.save('./tf_models/LunarLander.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tf_models/LunarLander.ckpt\n"
     ]
    }
   ],
   "source": [
    "model._model.load('./tf_models/LunarLander.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter, press squre '■' in top menu to quit animation\n",
    "try: evaluate(env, model, frames=float('inf'), eps=0.0, render=True)\n",
    "except KeyboardInterrupt: pass\n",
    "finally: env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected agent behaviour after 25000 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/lunarlander.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you enable plotting, then output after training should be in roughly as follows\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/lunarlander.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "* Q Values - is a plot of average q-values for set of states - this shows how q-values develop during training\n",
    "* Episode Rewards - individual rewards acquired (blue dots) and running average (yellow) over the training period\n",
    "* Trajectory - is set of state variables over time within one or more episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render HTML5 Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is for rendering the animations, ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_render(env, frames, episodes, eps, model, callback=None, trace=None, render=True, sleep=0):\n",
    "\n",
    "    rendered_frames = []\n",
    "    \n",
    "    def policy(st, model, eps):\n",
    "        if np.random.rand() > eps:\n",
    "            stack = np.stack([st])  # convert lazyframe to nn input shape [1, 84, 84, 4]\n",
    "            q_values = model.eval(stack)\n",
    "            return np.argmax(q_values)\n",
    "        else:\n",
    "            return env.action_space.sample()\n",
    "        \n",
    "    total_reward = 0\n",
    "    \n",
    "    tts_ = 0                                 # total time step\n",
    "    for e_ in itertools.count():             # count from 0 to infinity\n",
    "        \n",
    "        S = env.reset()\n",
    "        \n",
    "        if render:\n",
    "            rendered_frames.append(env.render(mode='rgb_array'))\n",
    "            #env.render()\n",
    "            time.sleep(sleep)\n",
    "        \n",
    "        for t_ in itertools.count():         # count from 0 to infinity\n",
    "            \n",
    "            # print(e_, t_)\n",
    "            \n",
    "            A = policy(S, model, eps)\n",
    "            \n",
    "            S_, R, done, info = env.step(A)\n",
    "            \n",
    "            #total_reward += info['full-reward']\n",
    "            \n",
    "            if render:\n",
    "                rendered_frames.append(env.render(mode='rgb_array'))\n",
    "                #env.render()\n",
    "                time.sleep(sleep)\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(tts_, e_, t_, S, A, R, done, eps, model, None, trace)\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            if frames is not None and tts_ >= frames:\n",
    "                return rendered_frames, total_reward\n",
    "                \n",
    "            S = S_\n",
    "                \n",
    "            tts_ += 1\n",
    "            \n",
    "        if episodes is not None and e_ >= episodes-1:\n",
    "            return rendered_frames, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(env, frames=None, episodes=1, eps=0.05, model=model, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_frames, total_reward = frames_render(env, frames=None, episodes=1, eps=0.05, model=model, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "\n",
    "fig = plt.figure(figsize=(rendered_frames[0].shape[1] / 72.0,\n",
    "                          rendered_frames[0].shape[0] / 72.0), dpi = 72)\n",
    "ax = fig.add_subplot(111);\n",
    "\n",
    "patch = ax.imshow(rendered_frames[0])\n",
    "\n",
    "plt.tick_params(axis='both', which='both', bottom=False, top=False, left=False,\n",
    "                right=False, labelbottom=False, labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(i):\n",
    "    patch.set_data(rendered_frames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(fig, animate, frames=len(rendered_frames), interval=20, repeat=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then:\n",
    "* r-click and download html5 video (will save as mp4)\n",
    "* convert to gif with: https://ezgif.com/video-to-gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
